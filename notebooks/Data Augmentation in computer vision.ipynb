{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introduction\n",
    "In this notebook, we're going to explore a data set consisting of simulated data from the ATLAS-experiment which is located at the Large Hadron Collider at CERN. We'll look at the distribution of the data and labels, and explore the many existing techniques for data augmentation in deep learning for computer vision. Then we'll train a model to see how well it can classify between the two labels: black holes and sphalerons - which are two theories in fundamental phyics with similar propeties. The data set was generated by Aurora Grefsrud, a PhD-student who is a part of a research group called HVL ATLAS group at the Western Norway University of Applied Sciences. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Setup\n",
    "We're using the PyTorch setup which is great at building and training deep neural networks, such as convolutional neural networks (CNNs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Sets up the data directories\n",
    "This section covers the setup of the environment for our machine learning project. We import the necessary modules, set up the directory structure, and load the data for our analysis.\n",
    "\n",
    "In the methods folder there is a dataloader.py file and a plotCreator.py file. The separate dataloader-file provides an interface to load and preprocess data into a format that can be fed into a neural network for training and validation. \n",
    "\n",
    "The separate plotCreator-file provides a modile that contains functions for creating visualizations of data, such as histograms and grayscale images. These can be useful in analyzing and understanding the data used in a neural network.\n",
    "\n",
    "Keeping them separate files improves the modularity, organization, and maintainability of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs a path to a directory that contains dataloader.py and plotCreator.py\n",
    "module_path = str(Path.cwd().parents[0] / \"methods\")\n",
    "\n",
    "# Checks to see if the directory is already in sys.path to avoid adding it multiple times.\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Imports all the functions defined in the dataloader.py\n",
    "from dataloader import *\n",
    "\n",
    "# Creates two file paths pointing to two HDF5 files\n",
    "data_path0 = str(Path.cwd().parents[0] / \"data\" / \"BH_n4_M10_res50_15000_events.h5\")\n",
    "data_path1 = str(Path.cwd().parents[0] / \"data\" / \"PP13-Sphaleron-THR9-FRZ15-NB0-NSUBPALL_res50_15000_events.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the two HDF5 data files and creates two NumPy arrays\n",
    "bhArray = dataToArray(data_path0)\n",
    "sphArray = dataToArray(data_path1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Inspect data\n",
    "What are the dimensions of our two numpy arrays, and how does this effect what we know about the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 50, 50, 3)\n",
      "(15000, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "# Prints the shape of the arrays\n",
    "print(bhArray.shape)\n",
    "print(sphArray.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuple represents the simulated data from the ATLAS detector at CERN, consisting of 15,000 2D histograms. The second and third elements of the tuple indicate that each histogram has a size of 50x50 pixels, and the final element indicates that the histograms are in RGB format with 3 channels. The channels correspond to the sub-detectors at ATLAS: EMcal, HCal, and tracks, represented respectively by the red, green, and blue color channels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the two arrays of data into a single array. It creates a combined dataset\n",
    "# that can be used for training the machine learning model to distinguish \n",
    "# between the two classes.\n",
    "dataArray = np.concatenate((bhArray, sphArray), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 50, 50, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dataArray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the dataArray tells us that the concatenation of the 'bhArray' and 'sphArray' was performed correctly, since it has the expected number of data points and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an array of length 30,000 with the first 15,000 elements set to 0 and the\n",
    "# second 15,000 elements set to 1. This corresponds to the two classes of data: black\n",
    "# hole (class 0) and sphaleron (class 1).\n",
    "labelsArray = np.concatenate((np.zeros(15_000),np.ones(15_000)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(labelsArray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the above cell tells us that we have managed to separate our arrays into two different arrays. The dataArray contains the features, which is the actual data, while the labelsArray only contains the labels. This makes it possible for the machine learning algorithm to utilize supervised machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "# Checks whether a CUDA-enabled GPU is available on the system where the code is running.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Create training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly splits the dataArray and labelsArray into two sets. The trainData and trainLabels\n",
    "# will be used to train the machine learning model, while the testData and testLabels will be \n",
    "# used to evaluate the performance of the model. \n",
    "trainData, testData, trainLabels, testLabels = train_test_split(dataArray, labelsArray, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the NumPy arrays into PyTorch tensors, preparing the data for use in PyTorch models.\n",
    "trainData = torch.from_numpy(trainData)\n",
    "testData = torch.from_numpy(testData)\n",
    "trainLabels = torch.from_numpy(trainLabels)\n",
    "testLabels = torch.from_numpy(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates PyTorch 'TensorDataset' objects that can be passed as input to PyTorch data loaders.\n",
    "train = torch.utils.data.TensorDataset(trainData, trainLabels)\n",
    "test = torch.utils.data.TensorDataset(testData, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates PyTorch 'DataLoader' objects from the train and test datasets.\n",
    "trainLoader = DataLoader(train, shuffle=True, batch_size=50)\n",
    "testLoader = DataLoader(test, shuffle=True, batch_size=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'shuffle = True' specifies that the data should be randomly shuffled before each epoch of training. This helps to ensure that the models sees different examples in each epoch, which can improve generalization and prevent overfitting.\n",
    "\n",
    "'batch_size = 50' specifiees that the data should be divided into batches of size 50. Batchin the data can help to improve the efficiency of training by allowing the model to process multiple examples in parallel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Neural network models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - A feedforward neural network\n",
    "Defines a simple feedforward neural network with three fully connected layers and ReLU activation functions. It is designed for classification tasks on input data with three features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, resolution, num_classes, stride=1):\n",
    "\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x:Tensor):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(x))\n",
    "        out = self.fc3(x)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - A convolutional neural network (CNN)\n",
    "Defines a convolutional neural network (CNN) with two convolutional layers, two max-pooling layers, and two fully connected layers, and is designed for classification tasks on 2D image data with 3 channels (RGB images). These types of neural networks are commonly used for image classification tasks because it is effective at learning spatial features and patterns from the image data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ConvModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(11*11*64, 128)\n",
    "        self.fc2 = nn.Linear(128,2)\n",
    "\n",
    "\n",
    "    def forward(self, x:Tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x) #to activate function above\n",
    "\n",
    "        x = F.max_pool2d(x,2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.max_pool2d(x,2)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courseProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3268914c2fe1960935dd3198c0234181b08c3d5a207fbbc9a9525ed71ff8441"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
